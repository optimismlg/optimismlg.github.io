{"meta":{"title":"leguan's notes","subtitle":null,"description":"生如夏花之绚烂 死如秋叶之静美","author":"乐冠","url":"http://yoursite.com"},"pages":[{"title":"分类","date":"2017-01-16T05:40:30.000Z","updated":"2017-01-16T05:45:52.000Z","comments":true,"path":"categories/index.html","permalink":"http://yoursite.com/categories/index.html","excerpt":"","text":""},{"title":"标签","date":"2017-01-22T15:23:56.000Z","updated":"2017-01-22T15:26:23.000Z","comments":true,"path":"tags/index.html","permalink":"http://yoursite.com/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"PRML读书笔记-ch0102-概率论","slug":"PRML-notes-Ch0102-Probability-Theory","date":"2017-01-16T11:26:24.000Z","updated":"2017-01-21T05:54:46.000Z","comments":true,"path":"2017/01/16/PRML-notes-Ch0102-Probability-Theory/","link":"","permalink":"http://yoursite.com/2017/01/16/PRML-notes-Ch0102-Probability-Theory/","excerpt":"概率论加法法则：$$p(X)=\\sum_{Y}p(X,Y)$$乘法法则：$$p(X,Y)=p(Y|X)P(X)$$其中，$p(X,Y)$为联合概率，$p(Y|X)$为条件概率","text":"概率论加法法则：$$p(X)=\\sum_{Y}p(X,Y)$$乘法法则：$$p(X,Y)=p(Y|X)P(X)$$其中，$p(X,Y)$为联合概率，$p(Y|X)$为条件概率由乘法法则可得贝叶斯公式：$$p(Y|X)=\\frac{p(X|Y)P(Y)}{p(X)}$$再由加法法则，贝叶斯公式的分母可以写成$$p(X)=\\sum_{Y}p(X,Y)=\\sum_{Y}p(X|Y)P(Y)$$如果$p(X,Y)=p(X)p(Y)$，那么$X,Y$是独立的。 概率密度概率密度函数对于实数$x$，概率密度$p(x)$定义为：当$\\delta x \\rightarrow 0$时，变量$x$落在区间$(x,x+\\delta x)$范围为的概率为$p(x)\\delta x$。$x$落在区间$(a,b)$的概率为： $$p(x \\in (a.b))= \\int_{a}^{b} p(x)dx$$概率密度函数，需要满足以下两个条件：$$p(x) \\ge 0$$$$\\int _{-\\infty}^{\\infty}p(x)dx=1$$ 累计分布函数累计分布函数定义为：$$P(z)=\\int_{-\\infty}^{z}p(x)dx$$满足$P’(x)=p(x)$。 联合概率密度对于多个变量$x_{1},\\dots,x_{D}$（用$\\mathbf x$表示），定义联合概率密度为$p(\\mathbf x)=p(x_{1},\\dots,x_{D})$，满足当$\\mathbf x$落在一个包含$\\mathbf x$的足够小空间$\\delta \\mathbf x$中时，其概率为$p(\\mathbf x)\\delta\\mathbf x$，也需要满足$$p(\\mathbf x) \\ge 0$$$$\\int _{-\\infty}^{\\infty}p(\\mathbf x)d\\mathbf x=1$$ 期望和方差期望函数$f(x)$在概率密度函数$p(x)$下的均值叫做$f(x)$的期望。离散分布下，定义为：$$\\mathbb{E}[f]=\\sum_{x}p(x)f(x)$$连续分布下，定义为：$$\\mathbb{E}[f]=\\int_p(x)f(x)$$ 条件期望多元函数可以对其中一个参数求期望，例如$\\mathbb{E}_{x}[f(x,y)]$是函数$f(x,y)$在概率密度$f(x)$上的期望。考虑$f(x,y)$在条件分布$p(x|y)$下的条件期望。当$x$是离散变量时，定义为$$\\mathbb{E}_{x}[f(|y)]=\\sum_{x}p(x|y)f(x)$$ 方差$f(x)$的方差定义为$$var[f]=\\mathbb{E}[(f(x)-\\mathbb{E}[f(x)])^2]$$平方展开后，可以写为$$var[f]=\\mathbb{E}[f(x^2)]-(\\mathbb{E}f(x))^2$$特别地，考虑$x$本身的方差$$var[x]=\\mathbb{E}[x^2]-\\mathbb{E}[x]^2$$ 协方差对于两个随机变量$x,y$，协方差定义为：$$\\begin{split}cov[x,y]&amp;=\\mathbb{E}_{x,y}[{x-\\mathbb{E}[x]}{y-\\mathbb{E}[y]}] \\\\&amp;=\\mathbb{E}_{x,y}[xy]-\\mathbb{E}[x]\\mathbb{E}[y]\\end{split}$$当变量$x,y$独立时，协方差为0. 协方差矩阵对于两个随机向量$\\mathbf{x},\\mathbf{y}$，其协方差为矩阵：$$\\begin{split}cov[\\mathbf{x},\\mathbf{y}]&amp;=\\mathbb{E}_{\\mathbf{x},\\mathbf{y}}[{\\mathbf{x}-\\mathbb{E}[\\mathbf{x}]}{\\mathbf{y}^{T}-\\mathbb{E}[\\mathbf{y}^{T}]}] \\\\&amp;=\\mathbb{E}_{\\mathbf{x},\\mathbf{y}}[\\mathbf{x}\\mathbf{y}^{T}]-\\mathbb{E}[\\mathbf{x}]\\mathbb{E}[\\mathbf{y}^{T}]\\end{split}$$ 贝叶斯概率假设我们有一组模型参数$\\mathbf{w}$，先验概率分布为$p(\\mathbf{w})$，$\\mathcal{D}={t_{1},\\dots,t_{N}}$为一组观测数据，这组数据在$\\mathbf{w}$的条件概率分布为$p(\\mathcal{D}|\\mathbf{w})$。贝叶斯公式告诉我们：$$p(\\mathbf{w}|\\mathcal{D})=\\frac{p(\\mathcal{D}|\\mathbf{w})p(\\mathbf{w})}{p(\\mathcal{D})}$$ 似然函数$p(\\mathcal{D}|\\mathbf{w})$可以看作是给定观测数据$\\mathcal{D}$的情况下关于参数$\\mathbf{w}$的一个函数，叫做似然函数(likelihood function)，它反映了给定参数向量$\\mathbf{w}$的情况下，生成这组观测数据的可能性。 上面的贝叶斯也可以写为$$posterior\\varpropto likelihood \\times prior$$这三个量都是$\\mathbf{w}$的函数。 在贝叶斯学派和频率学派眼中，似然函数$p(\\mathcal{D}|\\mathbf{w})$扮演了重要的角色，但是两者的认知各异。 频率学派认为，$\\mathbf{w}$是一个固定参数，由某些估计只来决定，误差的计算要考虑数据$\\mathcal{D}$的分布。 贝叶斯学派认为，数据集$\\mathcal{D}$是固定的，参数$\\mathbf{w}$的不确定性可用$\\mathbf{w}$的概率分布来表示。 频率估计：最大似然最常用的频率估计量是最大似然(maximum likelihood)，选择合适的$\\mathbf{w}$以最大化似然函数。在机器学习的文献中，似然函数的负对数叫做损失函数(error function)，因为负对数函数是单调递减函数，因此最大似然相当于最小化损失函数。 贝叶斯估计贝叶斯估计的一个重要观点是引入先验知识，根据后验概率决定参数$\\mathbf{w}$。 高斯分布高斯分布，又称为正态分布，定义为：$$\\mathcal{N}(x|\\mu,\\sigma^{2})=\\frac{1}{(2\\pi\\sigma^{2})^{1/2}}exp\\lbrace-\\frac{1}{2\\sigma^{2}}(x-\\mu)^2\\rbrace$$其中$\\mu$为均值，$\\sigma^2$为方差，方差的平方根$\\sigma$为标准差。 多维高斯分布对于$D$维的向量$\\mathbf{x}$，高斯分布定义为：$$\\mathcal{N}(x|\\mu,\\Sigma)=\\frac{1}{(2\\pi)^{D/2}}\\frac{1}{|\\Sigma|^{1/2}}exp\\lbrace-\\frac{1}{2}(\\mathbf{x}-\\mu)^{T}\\Sigma^{-1}(\\mathbf{x}-\\mu)\\rbrace$$其中，$D$维的向量$\\mu$是均值，$D\\times D$矩阵$\\Sigma$是方差，$|\\Sigma|$是其行列式。 最大似然估计假设我们有$N$个对$x$的观测数据，记为$\\mathbf{x}=(x_1,\\dots,x_N)^T$，这些数据是独立同分布(independent and identically distributed, i.i.d.)的，服从均值为$\\mu$，方差为$\\sigma^2$的高斯分布，则给定$\\mu$和$\\sigma^2$的情况下，概率密度为：$$p(\\mathbf{x}|\\mu,\\sigma^{2})=\\prod_{n=1}^{N}\\mathcal{N}(x_n|\\mu,\\sigma^2)$$当以上公式看作是关于$\\mu$和$\\sigma$的函数时，即为似然函数。为了确定未知参数，需要求解最大似然函数问题。由于对数函数是单调递增的，因此上式两边可取对数，得到$$\\ln p(\\mathbf{x}|\\mu,\\sigma^{2})=-\\frac{1}{2\\sigma^2}\\sum_{n=1}^{N}(x_N-\\mu)^2-\\frac{N}{2}\\ln \\sigma^2-\\frac{N}{2}\\ln (2\\pi)$$对$\\mu$最大化，得到最大似然解：$$\\mu_{ML}=\\frac{1}{N}\\sum_{n=1}^{N}x_n$$对$\\sigma^2$最大化，得到最大似然解：$$\\sigma_{ML}^2=\\frac{1}{N}\\sum_{n=1}^{N}(x_n-\\mu_{ML})^2$$但是这个解不是无偏的，可以计算期望$$\\mathbb{E}[\\mu_{ML}]=\\mu$$$$\\mathbb{E}[\\sigma_{ML}^2]=(\\frac{N-1}{N})\\sigma^2$$因此，方差的无偏估计为：$$\\tilde{\\sigma}^2=\\frac{N}{N-1}\\sigma_{ML}^2=\\frac{1}{N-1}\\sum_{n=1}^{N}(x_n-\\mu_{ML})^2$$ 重新理解曲线拟合重新考虑曲线拟合问题，设输入为$x=(x_1,\\dots,x_N)^T$，对应的目标值为$t=(t_1,\\dots,t_N)^T$，可以把$t$看作是以$y(x,\\mathbf{w})$为均值的高斯分布：$$p(t|x,\\mathbf{w},\\beta)=\\mathcal{N}(t|y(x,\\mathbf{w}),\\beta^{-1})$$ 最大似然我们可以使用最大似然方法从训练数据${\\mathbf{x},\\mathbf{t}}$中求解参数$\\mathbf{w}$和$\\beta$，设训练集数据是独立同分布的，似然函数为：$$p(\\mathbf{t}|\\mathbf{x},\\mathbf{w},\\beta)=\\prod_{n=1}^{N}\\mathcal{N}(t_n|y(x_n,w),\\beta^{-1})$$两边去对数有$$\\ln p(\\mathbf{t}|\\mathbf{x},\\mathbf{w},\\beta)=-\\frac{\\beta}{2}\\sum_{n=1}^{N}\\lbrace y(x_n,\\mathbf{w})\\rbrace^2+\\frac{N}{2}\\ln \\beta-\\frac{N}{2}\\ln (2\\pi)$$设最大似然解为$\\mathbf{w}_{ML}$，最大对数似然的问题等价为$$\\min \\frac{1}{2}\\sum_{n=1}^{N}\\lbrace y(x_n,\\mathbf{w})\\rbrace^2$$即为最小化平方误差和。再对精度$\\beta$求最大似然，有$$\\frac{1}{\\beta_{ML}}=\\frac{1}{N}\\sum_{n=1}^{N}\\lbrace y(x_n,\\mathbf{w}_{ML})-t_n\\rbrace^2$$因此，基于以上最大似然解，对于一个新的输入$x$，其输出$t$满足$$p(t|x,\\mathbf{w}_{ML},\\beta_{ML})=\\mathcal{N}(t|y(x,\\mathbf{w}_{ML}),\\beta_{ML}^{-1})$$ 最大后验假设系数$\\mathbf{w}$有一个先验知识$M+1$维多项式（加上常数项），$\\alpha$为控制参数，有$$p(\\mathbf{w}|\\alpha)=\\mathcal{N}(\\mathbf{w}|0,\\alpha^{-1}\\mathbf{I})=(\\frac{\\alpha}{2\\pi})^{(M+1)/2}exp\\lbrace-\\frac{\\alpha}{2}\\mathbf{w}^T\\mathbf{w}\\rbrace$$由贝叶斯公式，后验概率正比于先验概率和似然函数的乘积：$$p(\\mathbf{w}|\\mathbf{x},\\mathbf{t},\\alpha,\\beta)\\varpropto p(\\mathbf{w}|\\mathbf{x},\\mathbf{t},w,\\beta)p(w|\\alpha)$$我们可以通过最大化后验概率(maximum posterior, MAP)来确定参数$\\mathbf{w}$的值，对上式求对数，去除跟$\\mathbf{w}$无关项，得到$$\\max \\frac{\\beta}{2}\\sum_{n=1}^{N}\\lbrace y(x_n,\\mathbf{w})\\rbrace^2+\\frac{\\alpha}{2}\\mathbf{w}^T\\mathbf{w}$$因此，MAP的结果等价于均方误差加上二次正则化，其中正则参数 $\\lambda=\\alpha/\\beta$ 贝叶斯曲线拟合虽然在MAP中我们引入了先验分布，但是本质上它是单点估计。一个完全的贝叶斯估计要求对$\\mathbf{w}$的所有值进行积分。对于多项式拟合问题，给定训练集$\\mathbf{x}$和$\\mathbf{t}$，对于一个新的测试样例$x$，目标值为$t$，考虑分布$p(t|x,\\mathbf{x},\\mathbf{t})$：$$p(t|x,\\mathbf{x},\\mathbf{t})=\\int p(t|x,\\mathbf{w})p(\\mathbf{w}|\\mathbf{x},\\mathbf{t})dw$$其中$p(t|x,\\mathbf{w})$是给定的高斯分布，$p(\\mathbf{w}|\\mathbf{x},\\mathbf{t})$是训练集的后验概率（高斯分布），因此上式也是高斯分布，可以写成$$p(t|x,\\mathbf{x},\\mathbf{t})=\\mathcal{N}(t|m(x)),s^2(x))$$","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/categories/机器学习/"}],"tags":[]},{"title":"PRML读书笔记-ch0101-多项式拟合","slug":"PRML-notes-Ch0101-Polynomial-Curve-Fitting","date":"2017-01-15T13:48:23.000Z","updated":"2017-01-21T05:55:01.000Z","comments":true,"path":"2017/01/15/PRML-notes-Ch0101-Polynomial-Curve-Fitting/","link":"","permalink":"http://yoursite.com/2017/01/15/PRML-notes-Ch0101-Polynomial-Curve-Fitting/","excerpt":"多项式拟合假设我们有N组(x,t)训练集，记$\\mathbf{x}=(x_{1},x_{2},\\dots,x_{n})^{T}$，$\\mathbf{t}=(t_{1},t_{2},\\dots,t_{n})^{T}$，我们的目标是利用这个训练集，预测对新输入变量$\\hat{x}$的目标值$\\hat{t}$。 考虑使用多项式拟合$$y(x,\\mathbf{w})=w_{0}+w_{1}x+w_{2}x^{2}+\\dots+w_{M}x^{M}=\\sum_{j=0}^{M}w_{j}x^{j} \\tag{1}$$","text":"多项式拟合假设我们有N组(x,t)训练集，记$\\mathbf{x}=(x_{1},x_{2},\\dots,x_{n})^{T}$，$\\mathbf{t}=(t_{1},t_{2},\\dots,t_{n})^{T}$，我们的目标是利用这个训练集，预测对新输入变量$\\hat{x}$的目标值$\\hat{t}$。 考虑使用多项式拟合$$y(x,\\mathbf{w})=w_{0}+w_{1}x+w_{2}x^{2}+\\dots+w_{M}x^{M}=\\sum_{j=0}^{M}w_{j}x^{j} \\tag{1}$$这些多项式的系数可以通过我们的数据拟合得到，即在训练集上最小化一个关于$y(x,\\mathbf{w})$和 $t$ 的损失函数。常见的一个损失函数是平方误差和，定义为：$$E(\\mathbf{w})=\\frac{1}{2}\\sum_{n=1}^{N}{y(x,\\mathbf{w})-t_{n}}^{2} \\tag{2}$$ 为了控制过拟合现象，可以采用正则化（regulariztion）方法，增加一个惩罚项，防止系数达到偏大的值。一个最常用的正则项是平方正则项，即控制所有参数的平方和大小，$$\\tilde E(\\mathbf{w})=\\frac{1}{2}\\sum_{n=1}^{N}{y(x_{n},\\mathbf{w})-t_{n}}^{2}+\\frac{\\lambda}{2}\\lVert\\mathbf{w}\\lVert^{2} \\tag{3}$$ 其中$\\lVert\\mathbf{w}\\lVert^{2}=w_{0}^{2}+w_{1}^{2}+\\dots+w_{m}^{2}$，$\\lambda$是控制正则项和误差项的相对重要性。","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/categories/机器学习/"}],"tags":[]},{"title":"Hexo+github搭建个人博客","slug":"Build-person-blog-with-hexo-and-github","date":"2017-01-15T08:39:42.000Z","updated":"2017-01-22T15:39:03.000Z","comments":true,"path":"2017/01/15/Build-person-blog-with-hexo-and-github/","link":"","permalink":"http://yoursite.com/2017/01/15/Build-person-blog-with-hexo-and-github/","excerpt":"工作已3年有余，国企的慢节奏已荒废了不少技术，经历了各种焦虑、迷茫，下定决心要在闲暇之余积累技术知识。在学习Machine Learning的过程中，看到一篇博文，博客的风格甚是喜欢，因此动了念头搭建自己的博客。一来当作是技术学习的笔记整理，提，二来用于记录生活的点滴和个人感悟。目前，博客托管在github中，未来想申请个人域名来部署。 Hexo是个轻量级的基于Nodejs的静态博客框架，好像是个台湾人写的。我是在macbook上安装的hexo，风格采用的是NexT。整个部署过程包括本地安装、github配置与部署、NexT风格配置。","text":"工作已3年有余，国企的慢节奏已荒废了不少技术，经历了各种焦虑、迷茫，下定决心要在闲暇之余积累技术知识。在学习Machine Learning的过程中，看到一篇博文，博客的风格甚是喜欢，因此动了念头搭建自己的博客。一来当作是技术学习的笔记整理，提，二来用于记录生活的点滴和个人感悟。目前，博客托管在github中，未来想申请个人域名来部署。 Hexo是个轻量级的基于Nodejs的静态博客框架，好像是个台湾人写的。我是在macbook上安装的hexo，风格采用的是NexT。整个部署过程包括本地安装、github配置与部署、NexT风格配置。 本地安装配置环境Hexo是用nodejs来生成页面的，因此需要安装node环境，可到官方网站下载pkg文件安装。 我们需要将博客部署在github上，因此需要安装git，同时申请github账号。 正式安装HexoNode和Git都安装好后，在mac终端中执行如下命令1$ sudo npm install -g hexo 创建一个本地的文件夹blog，cd到blog里执行hexoc初始化命令1$ hexo init 这时，hexo的源文件已经下载到blog文件夹中，安装完毕。 然后再执行如下命令，生成静态页面 1$ hexo generate 启动本地服务，进行本地预览模式 1$ hexo server 浏览器输入 http://localhost:4000，可以看到hexo初始页面。本地已经设置好了，接下来需要跟github进行关联。 Github配置与部署建立博客仓库在github上建立与用户名对应的仓库，仓库名为”username.github.io”，然后在本地blog目录下打开_config.yml文件，翻到最下面，改成如下所示： 1234deploy: type: git repository: https://github.com/optimismlg/optimismlg.github.io branch: master 部署执行以下命令后可以使用git部署 1$ npm install hexo-deployer-git --save 然后执行配置指令 1$ hexo deploy 此时，在浏览器输入 http://optimismlg.github.io/ 就可以显示博客了。 常用的命令： 12345$ hexo new \"postName\" #新建文章$ hexo new page \"pageName\" #新建页面$ hexo generate #生成静态页面至public目录$ hexo server #开启预览访问端口（默认端口4000，'ctrl + c'关闭server）$ hexo deploy #将.deploy目录部署到GitHub NexT主题配置NexT主题旨在于简洁优雅且易于使用，github的stars数超过6000，是一套流行的hexo主题。 在hexo中有两份主要的配置文件，其名称都是 _config.yml。 其中，一份位于站点根目录下，主要包含hexo本身的配置；另一份位于主题目录下，这份配置由主题作者提供，主要用于配置主题相关的选项。在以下说明中，将前者称为站点配置文件，后者称为主题配置文件。 安装NexT首先下载NexT主题，在终端下定位到hexo站点目录，clone NexT代码 12$ cd your-hexo-site$ git clone https://github.com/iissnan/hexo-theme-next themes/next 然后打开blog根目录下的站点配置文件，找到 theme字段，将其值更改为next： 1theme: next 然后可以启动hexo站点，在浏览器输入 http://localhost:4000 查看主题是否已经替换。 主题设定1、设置Scheme：NexT提供了三种外观模式，通过Scheme来配置。打开/themes/next下的主题配置文件，搜索scheme关键字，将你需用启用的scheme前面注释 # 即可。我采用了Mist主题。 123#scheme: Musescheme: Mist#scheme: Pisces 2、设置语言：编辑站点配置文件，将language字段设置成中文。 1language: zh-Hans 3、设置头像：编辑站点配置文件，新增字段avatar，值设置成头像的链接地址。 4、设置昵称和站点描述：编辑站点配置文件，分别设置author和description字段的值。 主题配置1、添加分类页面 在终端窗口下，定位到hexo站点目录下。使用 hexo new page 新建一个页面，命名为categories： 12$ cd blog$ hexo new page categories 在source/categories目录下的index.md中，修改： 123title: 分类date: 2017-01-16 13:40:30type: \"categories\" 在主题配置文件中取消注释： 1categories: /categories 在要分类的文章加入category属性： 1category: \"hexo\" 2、添加标签页面 在终端窗口下，定位到hexo站点目录下。使用 hexo new page 新建一个页面，命名为tags： 12$ cd blog$ hexo new page tags 在source/tags目录下的index.md中，修改： 123title: 标签date: 2017-01-22 23:23:56type: \"tags\" 在主题配置文件中取消注释： 1tags: /tags 在要分类的文章加入tags属性： 1tags: [hexo,github,NexT] 3、设置代码高亮主题 NexT提供了5种代码高亮主题，默认使用的是白色的normal主题，可切换为暗黑色。在主题配置文件中，找到highlight_theme，将值改为night 1highlight_theme: night 4、显示数学公式NexT借助于MathJax来显示数学公式，此选项默认关闭。为了显示数据公式，在主题配置文件中找到mathjax，enable选项设为true。 12mathjax: enable: true","categories":[{"name":"hexo","slug":"hexo","permalink":"http://yoursite.com/categories/hexo/"}],"tags":[{"name":"hexo","slug":"hexo","permalink":"http://yoursite.com/tags/hexo/"},{"name":"github","slug":"github","permalink":"http://yoursite.com/tags/github/"},{"name":"NexT","slug":"NexT","permalink":"http://yoursite.com/tags/NexT/"}]}]}